{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Scikit-learn flair detection",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ban1ZvJ6n3cr",
        "colab_type": "code",
        "outputId": "e13ddb43-6f76-4243-e166-44aaea05bd9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8edURFa-nOd3",
        "colab_type": "code",
        "outputId": "2dd5c2e7-9384-465e-ab3b-70ba2ae6be26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "'''Features'''\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "'''Classifiers'''\n",
        "from sklearn.dummy import DummyClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "\n",
        "'''Metrics/Evaluation'''\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc, confusion_matrix\n",
        "from scipy import interp\n",
        "from itertools import cycle\n",
        "\n",
        "'''Plotting'''\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_style('darkgrid')\n",
        "\n",
        "'''Display'''\n",
        "from IPython.core.display import display, HTML\n",
        "display(HTML(\"<style>.container { width:95% !important; }</style>\"))\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "pd.options.display.float_format = '{:,.2f}'.format"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<style>.container { width:95% !important; }</style>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XRBj-Lgzjhsz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
        "import itertools, string, operator, re, unicodedata, nltk\n",
        "from operator import itemgetter\n",
        "from nltk.corpus import wordnet\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import TweetTokenizer, RegexpTokenizer\n",
        "from bs4 import BeautifulSoup\n",
        "import numpy as np\n",
        "from itertools import combinations\n",
        "from gensim.models import Phrases\n",
        "from collections import Counter\n",
        "from sklearn.externals import joblib \n",
        "import pickle"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPSsgtuS4eAJ",
        "colab_type": "text"
      },
      "source": [
        "Functions for Data-cleaning and Pre-processing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpRo4h7Bm_tZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Contraction map\n",
        "c_dict = {\n",
        "  \"ain't\": \"am not\",\n",
        "  \"aren't\": \"are not\",\n",
        "  \"can't\": \"cannot\",\n",
        "  \"can't've\": \"cannot have\",\n",
        "  \"'cause\": \"because\",\n",
        "  \"could've\": \"could have\",\n",
        "  \"couldn't\": \"could not\",\n",
        "  \"couldn't've\": \"could not have\",\n",
        "  \"didn't\": \"did not\",\n",
        "  \"doesn't\": \"does not\",\n",
        "  \"don't\": \"do not\",\n",
        "  \"hadn't\": \"had not\",\n",
        "  \"hadn't've\": \"had not have\",\n",
        "  \"hasn't\": \"has not\",\n",
        "  \"haven't\": \"have not\",\n",
        "  \"he'd\": \"he would\",\n",
        "  \"he'd've\": \"he would have\",\n",
        "  \"he'll\": \"he will\",\n",
        "  \"he'll've\": \"he will have\",\n",
        "  \"he's\": \"he is\",\n",
        "  \"how'd\": \"how did\",\n",
        "  \"how'd'y\": \"how do you\",\n",
        "  \"how'll\": \"how will\",\n",
        "  \"how's\": \"how is\",\n",
        "  \"i'd\": \"I would\",\n",
        "  \"i'd've\": \"I would have\",\n",
        "  \"i'll\": \"I will\",\n",
        "  \"i'll've\": \"I will have\",\n",
        "  \"i'm\": \"I am\",\n",
        "  \"i've\": \"I have\",\n",
        "  \"isn't\": \"is not\",\n",
        "  \"it'd\": \"it had\",\n",
        "  \"it'd've\": \"it would have\",\n",
        "  \"it'll\": \"it will\",\n",
        "  \"it'll've\": \"it will have\",\n",
        "  \"it's\": \"it is\",\n",
        "  \"let's\": \"let us\",\n",
        "  \"ma'am\": \"madam\",\n",
        "  \"mayn't\": \"may not\",\n",
        "  \"might've\": \"might have\",\n",
        "  \"mightn't\": \"might not\",\n",
        "  \"mightn't've\": \"might not have\",\n",
        "  \"must've\": \"must have\",\n",
        "  \"mustn't\": \"must not\",\n",
        "  \"mustn't've\": \"must not have\",\n",
        "  \"needn't\": \"need not\",\n",
        "  \"needn't've\": \"need not have\",\n",
        "  \"o'clock\": \"of the clock\",\n",
        "  \"oughtn't\": \"ought not\",\n",
        "  \"oughtn't've\": \"ought not have\",\n",
        "  \"shan't\": \"shall not\",\n",
        "  \"sha'n't\": \"shall not\",\n",
        "  \"shan't've\": \"shall not have\",\n",
        "  \"she'd\": \"she would\",\n",
        "  \"she'd've\": \"she would have\",\n",
        "  \"she'll\": \"she will\",\n",
        "  \"she'll've\": \"she will have\",\n",
        "  \"she's\": \"she is\",\n",
        "  \"should've\": \"should have\",\n",
        "  \"shouldn't\": \"should not\",\n",
        "  \"shouldn't've\": \"should not have\",\n",
        "  \"so've\": \"so have\",\n",
        "  \"so's\": \"so is\",\n",
        "  \"that'd\": \"that would\",\n",
        "  \"that'd've\": \"that would have\",\n",
        "  \"that's\": \"that is\",\n",
        "  \"there'd\": \"there had\",\n",
        "  \"there'd've\": \"there would have\",\n",
        "  \"there's\": \"there is\",\n",
        "  \"they'd\": \"they would\",\n",
        "  \"they'd've\": \"they would have\",\n",
        "  \"they'll\": \"they will\",\n",
        "  \"they'll've\": \"they will have\",\n",
        "  \"they're\": \"they are\",\n",
        "  \"they've\": \"they have\",\n",
        "  \"to've\": \"to have\",\n",
        "  \"wasn't\": \"was not\",\n",
        "  \"we'd\": \"we had\",\n",
        "  \"we'd've\": \"we would have\",\n",
        "  \"we'll\": \"we will\",\n",
        "  \"we'll've\": \"we will have\",\n",
        "  \"we're\": \"we are\",\n",
        "  \"we've\": \"we have\",\n",
        "  \"weren't\": \"were not\",\n",
        "  \"what'll\": \"what will\",\n",
        "  \"what'll've\": \"what will have\",\n",
        "  \"what're\": \"what are\",\n",
        "  \"what's\": \"what is\",\n",
        "  \"what've\": \"what have\",\n",
        "  \"when's\": \"when is\",\n",
        "  \"when've\": \"when have\",\n",
        "  \"where'd\": \"where did\",\n",
        "  \"where's\": \"where is\",\n",
        "  \"where've\": \"where have\",\n",
        "  \"who'll\": \"who will\",\n",
        "  \"who'll've\": \"who will have\",\n",
        "  \"who's\": \"who is\",\n",
        "  \"who've\": \"who have\",\n",
        "  \"why's\": \"why is\",\n",
        "  \"why've\": \"why have\",\n",
        "  \"will've\": \"will have\",\n",
        "  \"won't\": \"will not\",\n",
        "  \"won't've\": \"will not have\",\n",
        "  \"would've\": \"would have\",\n",
        "  \"wouldn't\": \"would not\",\n",
        "  \"wouldn't've\": \"would not have\",\n",
        "  \"y'all\": \"you all\",\n",
        "  \"y'alls\": \"you alls\",\n",
        "  \"y'all'd\": \"you all would\",\n",
        "  \"y'all'd've\": \"you all would have\",\n",
        "  \"y'all're\": \"you all are\",\n",
        "  \"y'all've\": \"you all have\",\n",
        "  \"you'd\": \"you had\",\n",
        "  \"you'd've\": \"you would have\",\n",
        "  \"you'll\": \"you you will\",\n",
        "  \"you'll've\": \"you you will have\",\n",
        "  \"you're\": \"you are\",\n",
        "  \"you've\": \"you have\",\n",
        "  \"no_text\": \" \"\n",
        "}\n",
        "\n",
        "c_re = re.compile('(%s)' % '|'.join(c_dict.keys()))\n",
        "\n",
        "add_stop = ['', ' ', 'say', 's', 'u', 'ap', 'afp', '...', 'n', '\\\\']\n",
        "\n",
        "stop_words = ENGLISH_STOP_WORDS.union(add_stop)\n",
        "\n",
        "tokenizer = TweetTokenizer()\n",
        "pattern = r\"(?u)\\b\\w\\w+\\b\" \n",
        "\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "punc = list(set(string.punctuation))\n",
        "\n",
        "def casual_tokenizer(text): #Splits words on white spaces (leaves contractions intact) and splits out trailing punctuation\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "#Function to replace the nltk pos tags with the corresponding wordnet pos tag to use the wordnet lemmatizer\n",
        "def get_word_net_pos(treebank_tag):\n",
        "    if treebank_tag.startswith('J'):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith('V'):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith('N'):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith('R'):\n",
        "        return wordnet.ADV\n",
        "    else:\n",
        "        return None\n",
        "    \n",
        "def lemma_wordnet(tagged_text):\n",
        "    final = []\n",
        "    for word, tag in tagged_text:\n",
        "        wordnet_tag = get_word_net_pos(tag)\n",
        "        if wordnet_tag is None:\n",
        "            final.append(lemmatizer.lemmatize(word))\n",
        "        else:\n",
        "            final.append(lemmatizer.lemmatize(word, pos=wordnet_tag))\n",
        "    return final\n",
        "\n",
        "def expandContractions(text, c_re=c_re):\n",
        "    def replace(match):\n",
        "        return c_dict[match.group(0)]\n",
        "    return c_re.sub(replace, text)\n",
        "\n",
        "def remove_html(text):\n",
        "    soup = BeautifulSoup(text, \"html5lib\")\n",
        "    tags_del = soup.get_text()\n",
        "    uni = unicodedata.normalize(\"NFKD\", tags_del)\n",
        "    bracket_del = re.sub(r'\\[.*?\\]', '  ', uni)\n",
        "    apostrphe = re.sub('’', \"'\", bracket_del)\n",
        "    string = apostrphe.replace('\\r','  ')\n",
        "    string = string.replace('\\n','  ')\n",
        "    extra_space = re.sub(' +',' ', string)\n",
        "    return extra_space\n",
        "\n",
        "def process_text(text):\n",
        "    soup = BeautifulSoup(text, \"lxml\")\n",
        "    tags_del = soup.get_text()\n",
        "    no_html = re.sub('<[^>]*>', '', tags_del)\n",
        "    tokenized = casual_tokenizer(no_html)\n",
        "    lower = [item.lower() for item in tokenized]\n",
        "    decontract = [expandContractions(item, c_re=c_re) for item in lower]\n",
        "    tagged = nltk.pos_tag(decontract)\n",
        "    lemma = lemma_wordnet(tagged)\n",
        "    no_num = [re.sub('[0-9]+', '', each) for each in lemma]\n",
        "    no_punc = [w for w in no_num if w not in punc]\n",
        "    no_stop = [w for w in no_punc if w not in stop_words]\n",
        "    return no_stop\n",
        "\n",
        "def word_count(text):\n",
        "    return len(str(text).split(' '))\n",
        "\n",
        "def word_freq(clean_text_list, top_n):\n",
        "    \"\"\"\n",
        "    Word Frequency\n",
        "    \"\"\"\n",
        "    flat = [item for sublist in clean_text_list for item in sublist]\n",
        "    with_counts = Counter(flat)\n",
        "    top = with_counts.most_common(top_n)\n",
        "    word = [each[0] for each in top]\n",
        "    num = [each[1] for each in top]\n",
        "    return pd.DataFrame([word, num]).T\n",
        "\n",
        "def word_freq_bigrams(clean_text_list, top_n):\n",
        "    \"\"\"\n",
        "    Word Frequency With Bigrams\n",
        "    \"\"\"\n",
        "    bigram_model = Phrases(clean_text_list, min_count=2, threshold=1)\n",
        "    w_bigrams = list(bigram_model[clean_text_list])\n",
        "    flat_w_bigrams = [item for sublist in w_bigrams for item in sublist]\n",
        "    with_counts = Counter(flat_w_bigrams)\n",
        "    top = with_counts.most_common(top_n)\n",
        "    word = [each[0] for each in top]\n",
        "    num = [each[1] for each in top]\n",
        "    return pd.DataFrame([word, num]).T\n",
        "\n",
        "\n",
        "def bigram_freq(clean_text_list, top_n):\n",
        "    bigram_model = Phrases(clean_text_list, min_count=2, threshold=1)\n",
        "    w_bigrams = list(bigram_model[clean_text_list])\n",
        "    flat_w_bigrams = [item for sublist in w_bigrams for item in sublist]\n",
        "    bigrams = []\n",
        "    for each in flat_w_bigrams:\n",
        "        if '_' in each:\n",
        "            bigrams.append(each)\n",
        "    counts = Counter(bigrams)\n",
        "    top = counts.most_common(top_n)\n",
        "    word = [each[0] for each in top]\n",
        "    num = [each[1] for each in top]\n",
        "    return pd.DataFrame([word, num]).T"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3BgPHk3nclu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = pd.read_csv('Posts-data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HuXgZF064pdH",
        "colab_type": "text"
      },
      "source": [
        "Pre-process 'text' field of our data. We need to eliminate 'no_text' while cleaning. So we replaced 'no_text' with space in c_dict."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7NlgJVHqnwqn",
        "colab_type": "code",
        "outputId": "f357f790-5731-446a-f0f4-d3790ef27501",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "df['clean_text'] = df['text'].apply(process_text)\n",
        "df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>flair</th>\n",
              "      <th>clean_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Ordering and buying stuff online right now Hi!...</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>[ordering, buying, stuff, online, right, hi, a...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Are there any astrologers on reddit? Did you w...</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>[astrologer, reddit, warn, regular, customer, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Is there any good smartphone with 3 or even 4 ...</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>[good, smartphone, sim, card, hello, look, goo...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Required: Medium of Instruction Letter (MOI) f...</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>[require, medium, instruction, letter, moi, mu...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>American Wanting to Travel to India - Tips and...</td>\n",
              "      <td>AskIndia</td>\n",
              "      <td>[american, want, travel, india, tip, advice, h...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  ...                                         clean_text\n",
              "0  Ordering and buying stuff online right now Hi!...  ...  [ordering, buying, stuff, online, right, hi, a...\n",
              "1  Are there any astrologers on reddit? Did you w...  ...  [astrologer, reddit, warn, regular, customer, ...\n",
              "2  Is there any good smartphone with 3 or even 4 ...  ...  [good, smartphone, sim, card, hello, look, goo...\n",
              "3  Required: Medium of Instruction Letter (MOI) f...  ...  [require, medium, instruction, letter, moi, mu...\n",
              "4  American Wanting to Travel to India - Tips and...  ...  [american, want, travel, india, tip, advice, h...\n",
              "\n",
              "[5 rows x 3 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M2meRjM7oJo5",
        "colab_type": "code",
        "outputId": "f73c0275-fce1-447a-cf21-dcd74f980651",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 669
        }
      },
      "source": [
        "# Counting word-frequency or most occuring words in clean-text field.\n",
        "cl_text_list = df['clean_text'].tolist() \n",
        "wf = word_freq(cl_text_list, 20)\n",
        "wf.head(20)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>no_text</td>\n",
              "      <td>652</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>india</td>\n",
              "      <td>352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>’</td>\n",
              "      <td>232</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>indian</td>\n",
              "      <td>221</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>time</td>\n",
              "      <td>155</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>people</td>\n",
              "      <td>151</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>covid</td>\n",
              "      <td>125</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>help</td>\n",
              "      <td>120</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>like</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>make</td>\n",
              "      <td>113</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>use</td>\n",
              "      <td>104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>want</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>just</td>\n",
              "      <td>101</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>country</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>need</td>\n",
              "      <td>100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>year</td>\n",
              "      <td>97</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>china</td>\n",
              "      <td>94</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>know</td>\n",
              "      <td>91</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>good</td>\n",
              "      <td>89</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>day</td>\n",
              "      <td>88</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "          0    1\n",
              "0   no_text  652\n",
              "1     india  352\n",
              "2         ’  232\n",
              "3    indian  221\n",
              "4      time  155\n",
              "5    people  151\n",
              "6     covid  125\n",
              "7      help  120\n",
              "8      like  113\n",
              "9      make  113\n",
              "10      use  104\n",
              "11     want  101\n",
              "12     just  101\n",
              "13  country  100\n",
              "14     need  100\n",
              "15     year   97\n",
              "16    china   94\n",
              "17     know   91\n",
              "18     good   89\n",
              "19      day   88"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YoO3czU7ofDR",
        "colab_type": "code",
        "outputId": "ca5b7585-a619-4f8c-f1de-f95274866bcd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 421
        }
      },
      "source": [
        "df['word_count'] = df['text'].apply(word_count)\n",
        "avg_wc = df.groupby('flair').mean().reset_index()\n",
        "avg_wc[['flair','word_count']]"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>flair</th>\n",
              "      <th>word_count</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>AskIndia</td>\n",
              "      <td>120.05</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Business/Finance</td>\n",
              "      <td>49.08</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Coronavirus</td>\n",
              "      <td>22.01</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Food</td>\n",
              "      <td>33.34</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Non-Political</td>\n",
              "      <td>25.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Photography</td>\n",
              "      <td>13.58</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>Policy/Economy</td>\n",
              "      <td>103.26</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>Politics</td>\n",
              "      <td>40.22</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>Scheduled</td>\n",
              "      <td>110.67</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>Science/Technology</td>\n",
              "      <td>65.27</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>Sports</td>\n",
              "      <td>25.38</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>[R]eddiquette</td>\n",
              "      <td>152.29</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                 flair  word_count\n",
              "0             AskIndia      120.05\n",
              "1     Business/Finance       49.08\n",
              "2          Coronavirus       22.01\n",
              "3                 Food       33.34\n",
              "4        Non-Political       25.38\n",
              "5          Photography       13.58\n",
              "6       Policy/Economy      103.26\n",
              "7             Politics       40.22\n",
              "8            Scheduled      110.67\n",
              "9   Science/Technology       65.27\n",
              "10              Sports       25.38\n",
              "11       [R]eddiquette      152.29"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-5qqkWEooNF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "LE = LabelEncoder()\n",
        "df['flair_num'] = LE.fit_transform(df['flair'].astype(str))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SwImJU3gouAD",
        "colab_type": "code",
        "outputId": "7e4b3338-816f-47e4-d936-1a542790321f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df.flair_num.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "6     122\n",
              "7     120\n",
              "4     120\n",
              "2     120\n",
              "0     120\n",
              "5      67\n",
              "1      59\n",
              "9      37\n",
              "11     34\n",
              "10     34\n",
              "3      29\n",
              "8      21\n",
              "Name: flair_num, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-T5GYjAtN1Z",
        "colab_type": "code",
        "outputId": "5898d349-c3bb-4e16-9e31-6de633f3aff3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        }
      },
      "source": [
        "df.flair.value_counts()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Policy/Economy        122\n",
              "Non-Political         120\n",
              "Politics              120\n",
              "Coronavirus           120\n",
              "AskIndia              120\n",
              "Photography            67\n",
              "Business/Finance       59\n",
              "Science/Technology     37\n",
              "[R]eddiquette          34\n",
              "Sports                 34\n",
              "Food                   29\n",
              "Scheduled              21\n",
              "Name: flair, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7dMwY6P7Jlt",
        "colab_type": "text"
      },
      "source": [
        "Tfidf vectorizer is used for building features from our clean_text field. For building features we are using unigrams and bigrams only. ngram_range = (1, 2)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EjQO13Uloys0",
        "colab_type": "code",
        "outputId": "1db3bf4e-748c-4c78-91d6-38b9b0c6b803",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "texts = df['clean_text'].astype('str')\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(ngram_range=(1, 2), \n",
        "                                   min_df = 2, \n",
        "                                   max_df = .95)\n",
        "\n",
        "X = tfidf_vectorizer.fit_transform(texts) #features\n",
        "y = df['flair_num'].values #target\n",
        "\n",
        "print (X.shape)\n",
        "print(y.shape)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(883, 9322)\n",
            "(883,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SFNhBYVC7oFi",
        "colab_type": "text"
      },
      "source": [
        "As our clean_text data field is not very big (consists majorly title data), we are truncating features to 10 features only. So, that later while deploying our model it can work properly."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2lr31ncgpONs",
        "colab_type": "code",
        "outputId": "b3f2fcc7-931b-49b4-ab09-a9ce61200802",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "lsa = TruncatedSVD(n_components=10, \n",
        "                   n_iter=10, \n",
        "                   random_state=3)\n",
        "\n",
        "X = lsa.fit_transform(X)\n",
        "X.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(883, 10)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObCR7aYZ8F-3",
        "colab_type": "text"
      },
      "source": [
        "Testing which Scikit ML model works best on our dataset. We built a dictionry of seven ml models."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AjnhFcjnpF-z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_dict = {'Dummy' : DummyClassifier(random_state=3),\n",
        "              'Stochastic Gradient Descent' : SGDClassifier(random_state=3, loss='log'),\n",
        "              'Random Forest': RandomForestClassifier(random_state=3), \n",
        "              'Decsision Tree': DecisionTreeClassifier(random_state=3),\n",
        "              'AdaBoost': AdaBoostClassifier(random_state=3), \n",
        "              'Gaussian Naive Bayes': GaussianNB(),\n",
        "              'K Nearest Neighbor': KNeighborsClassifier()}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INELxe1IpZcz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .3, shuffle = True, stratify = y, random_state = 3)\n",
        "# Expliciting data into trainting and test data."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cucXfCe-8eLn",
        "colab_type": "text"
      },
      "source": [
        "Compile all the model and result accuracy for each of the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C3AWaDjYqOKq",
        "colab_type": "code",
        "outputId": "f643f35f-6b4b-4e92-d84c-a83ff75be5d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        }
      },
      "source": [
        "def model_score_df(model_dict):\n",
        "  model_name, ac_score_list, p_score_list, r_score_list, f1_score_list = [], [], [], [], [] \n",
        "  for k,v in model_dict.items():\n",
        "    model_name.append(k) \n",
        "    v.fit(X_train, y_train) \n",
        "    y_pred = v.predict(X_test) \n",
        "    ac_score_list.append(accuracy_score(y_test, y_pred)) \n",
        "    p_score_list.append(precision_score(y_test, y_pred, average='macro')) \n",
        "    r_score_list.append(recall_score(y_test, y_pred, average='macro')) \n",
        "    f1_score_list.append(f1_score(y_test, y_pred, average='macro')) \n",
        "    model_comparison_df = pd.DataFrame([model_name, ac_score_list, p_score_list, r_score_list, f1_score_list]).T \n",
        "    model_comparison_df.columns = ['model_name', 'accuracy_score', 'precision_score', 'recall_score', 'f1_score'] \n",
        "    model_comparison_df = model_comparison_df.sort_values(by='f1_score', ascending=False) \n",
        "  return model_comparison_df\n",
        "\n",
        "model_score_df(model_dict)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>model_name</th>\n",
              "      <th>accuracy_score</th>\n",
              "      <th>precision_score</th>\n",
              "      <th>recall_score</th>\n",
              "      <th>f1_score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Random Forest</td>\n",
              "      <td>0.58</td>\n",
              "      <td>0.66</td>\n",
              "      <td>0.53</td>\n",
              "      <td>0.55</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Decsision Tree</td>\n",
              "      <td>0.51</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.48</td>\n",
              "      <td>0.48</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>K Nearest Neighbor</td>\n",
              "      <td>0.41</td>\n",
              "      <td>0.40</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.37</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>Gaussian Naive Bayes</td>\n",
              "      <td>0.37</td>\n",
              "      <td>0.38</td>\n",
              "      <td>0.33</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Stochastic Gradient Descent</td>\n",
              "      <td>0.43</td>\n",
              "      <td>0.36</td>\n",
              "      <td>0.35</td>\n",
              "      <td>0.32</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>AdaBoost</td>\n",
              "      <td>0.26</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.24</td>\n",
              "      <td>0.17</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Dummy</td>\n",
              "      <td>0.08</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.06</td>\n",
              "      <td>0.05</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                    model_name accuracy_score  ... recall_score f1_score\n",
              "2                Random Forest           0.58  ...         0.53     0.55\n",
              "3               Decsision Tree           0.51  ...         0.48     0.48\n",
              "6           K Nearest Neighbor           0.41  ...         0.38     0.37\n",
              "5         Gaussian Naive Bayes           0.37  ...         0.33     0.32\n",
              "1  Stochastic Gradient Descent           0.43  ...         0.35     0.32\n",
              "4                     AdaBoost           0.26  ...         0.24     0.17\n",
              "0                        Dummy           0.08  ...         0.06     0.05\n",
              "\n",
              "[7 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Syk8cCbb9K3Z",
        "colab_type": "text"
      },
      "source": [
        "As we can see from the above table, Random_Forest ML model is working best."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kDzfc7io9aha",
        "colab_type": "text"
      },
      "source": [
        "Train our Random_Forest model and exporting model as pickle file."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8HSBj3XKrqOM",
        "colab_type": "code",
        "outputId": "905adb20-bc3c-4ccd-e7df-8a9aa27a8b87",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "RFC = RandomForestClassifier(random_state=3)\n",
        "RFC.fit(X_train, y_train)\n",
        "pickle_out = open(\"model.pickle\",\"wb\")\n",
        "pickle.dump(RFC, pickle_out)\n",
        "pickle_out.close()\n",
        "pickle_in = open(\"model.pickle\",\"rb\")\n",
        "RFC_from_pickle = pickle.load(pickle_in)\n",
        "RFC_from_pickle.predict(X_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 2,  7,  7,  0,  6,  7,  2, 10,  0,  4,  5,  4, 11,  6,  0,  4,  6,\n",
              "        7,  2,  2,  0,  7,  7,  4,  4,  2,  5,  5,  7,  4,  6,  8,  6,  8,\n",
              "        8,  0,  2,  7,  1,  8,  4,  2,  9,  7,  1,  6,  6,  1,  6,  4,  1,\n",
              "        0,  1,  0,  5,  2,  4,  0,  6,  4,  0,  7,  5,  7,  4,  0,  4, 11,\n",
              "        0,  4,  4,  0,  2,  0,  6,  6,  9,  6,  0,  0,  4,  2,  4,  7,  4,\n",
              "        6,  0,  0,  5,  2,  7,  4,  0,  0,  7,  0,  2,  6,  7,  1,  0,  5,\n",
              "        4,  8,  6,  0, 10, 11,  4,  5,  4,  7,  0,  0,  6,  5,  2,  6,  2,\n",
              "        0,  6,  4,  7,  5,  0,  4,  3,  2,  4,  6,  0,  5,  2, 10,  5,  7,\n",
              "        4,  4,  6,  6,  7,  0,  0,  7,  0,  4,  4,  7,  0,  7,  8,  2,  6,\n",
              "        2,  7,  1,  2,  4,  6,  7,  6,  7,  6,  0,  5,  6,  4,  4,  2,  2,\n",
              "        2,  2,  4,  0,  6,  2,  2,  4, 11,  7,  0,  2,  4,  4,  9,  7,  7,\n",
              "       11,  6,  4,  1,  1,  0,  4,  2,  2,  6,  7,  2,  6,  7, 11,  5,  6,\n",
              "        4,  6,  4,  7,  7,  2,  5,  9,  0,  7,  2,  1,  7,  4,  7,  6,  2,\n",
              "        4,  6,  0,  7,  4, 10,  4,  0,  5,  2,  2,  4,  0,  4,  6,  6,  0,\n",
              "        4,  4,  4,  7,  4, 10,  5,  4,  7,  0,  2,  5,  7, 10,  6,  7,  0,\n",
              "        2,  2,  4,  0,  4,  0,  3,  0,  7,  5])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    }
  ]
}